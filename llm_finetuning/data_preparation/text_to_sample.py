import multiprocessing
import logging
from typing import List, Any, Tuple

# --- Constants ---
DEFAULT_MAX_TOKENS_PER_SAMPLE = 490
# Renamed from chunk_size in original split_text_into_samples to avoid confusion
DEFAULT_INTERNAL_PROCESSING_CHUNK_SIZE = 1_000_000
DEFAULT_NUM_PROCESSES = 12

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(process)d - %(levelname)s - %(message)s')


def split_text_into_samples(
    tokenizer: Any,
    text: str,
    max_samples: int, # Target number of samples for this chunk
    max_tokens_per_sample: int = DEFAULT_MAX_TOKENS_PER_SAMPLE,
    internal_processing_chunk_size: int = DEFAULT_INTERNAL_PROCESSING_CHUNK_SIZE,
) -> List[str]:
    """
    Splits a given text into samples, where each sample does not exceed
    `max_tokens_per_sample`. Stops when `max_samples` are generated.

    Args:
        tokenizer (Any): The tokenizer object with an `encode` method.
        text (str): The input text chunk to be split into samples.
        max_samples (int): The maximum number of samples to generate from this text chunk.
        max_tokens_per_sample (int): The maximum number of tokens allowed in each sample.
        internal_processing_chunk_size (int): Size of sub-chunks to process internally
                                              to manage memory (mostly relevant for very large texts).

    Returns:
        List[str]: A list of generated text samples.
    """
    samples: List[str] = []
    current_sample_words: List[str] = []
    current_token_count: int = 0
    total_samples_generated: int = 0

    logging.debug(f"Starting sample generation. Target: {max_samples} samples, Max tokens/sample: {max_tokens_per_sample}")

    start_idx = 0
    while start_idx < len(text) and total_samples_generated < max_samples:
        end_idx = min(start_idx + internal_processing_chunk_size, len(text))
        internal_chunk = text[start_idx:end_idx]
        # Simple splitting by space, consider more robust tokenization if needed
        words = internal_chunk.split()

        for word in words:
            if not word: # Skip empty strings resulting from multiple spaces
                continue
            try:
                # Tokenize the current word (excluding special tokens)
                word_tokens = tokenizer.encode(word, add_special_tokens=False)
                word_token_count = len(word_tokens)
            except Exception as e:
                logging.warning(f"Could not tokenize word: '{word[:50]}...'. Skipping. Error: {e}")
                continue

            # Check if adding this word exceeds the max token limit for the current sample
            if current_token_count + word_token_count > max_tokens_per_sample:
                # Finalize the current sample if it's not empty
                if current_sample_words:
                    samples.append(" ".join(current_sample_words))
                    total_samples_generated += 1
                    logging.log(logging.DEBUG - 1, f"Generated sample {total_samples_generated}/{max_samples}") # Use lower level for frequent logs

                    # Check if we reached the max number of samples for this chunk
                    if total_samples_generated >= max_samples:
                        logging.debug(f"Reached max samples ({max_samples}) for this process.")
                        return samples

                # Start a new sample with the current word
                current_sample_words = [word]
                current_token_count = word_token_count
            else:
                # Add the word to the current sample
                current_sample_words.append(word)
                current_token_count += word_token_count

        # Move to the next internal processing chunk
        start_idx = end_idx

    # Append the last sample if it's not empty and we haven't reached the limit
    if current_sample_words and total_samples_generated < max_samples:
        samples.append(" ".join(current_sample_words))
        total_samples_generated += 1
        logging.log(logging.DEBUG - 1, f"Generated final sample {total_samples_generated}/{max_samples}")

    logging.debug(f"Finished sample generation for this process. Generated {total_samples_generated} samples.")
    return samples


def process_chunk_wrapper(args: Tuple[str, int, int, int, Any]) -> List[str]:
    """
    Wrapper function to unpack arguments for use with pool.map or pool.starmap.

    Args:
        args (Tuple): A tuple containing (text_chunk, num_samples,
                      max_tokens_per_sample, internal_chunk_size, tokenizer).

    Returns:
        List[str]: The list of samples generated by split_text_into_samples.
    """
    text_chunk, num_samples, max_tokens_per_sample, internal_chunk_size, tokenizer = args
    try:
        return split_text_into_samples(
            tokenizer=tokenizer,
            text=text_chunk,
            max_samples=num_samples,
            max_tokens_per_sample=max_tokens_per_sample,
            internal_processing_chunk_size=internal_chunk_size,
        )
    except Exception as e:
        # Log error from within the worker process
        logging.error(f"Error in process_chunk_wrapper: {e}", exc_info=True)
        return [] # Return empty list on error


def distribute_processing(
    all_text: str,
    total_samples: int,
    tokenizer: Any,
    max_tokens_per_sample: int = DEFAULT_MAX_TOKENS_PER_SAMPLE,
    internal_processing_chunk_size: int = DEFAULT_INTERNAL_PROCESSING_CHUNK_SIZE,
    num_processes: int = DEFAULT_NUM_PROCESSES,
) -> List[str]:
    """
    Distributes the task of splitting text into samples across multiple processes.

    Args:
        all_text (str): The entire text corpus to process.
        total_samples (int): The total desired number of samples across all processes.
        tokenizer (Any): The tokenizer object.
        max_tokens_per_sample (int): Maximum tokens per sample.
        internal_processing_chunk_size (int): Internal chunk size for `split_text_into_samples`.
        num_processes (int): The number of worker processes to spawn.

    Returns:
        List[str]: A list containing all generated samples from all processes.
                   Returns an empty list if processing fails or no text is provided.
    """
    all_samples: List[str] = []
    try:
        text_length = len(all_text)
        if text_length == 0:
            logging.warning("Input text is empty. Returning empty list.")
            return []

        # Adjust num_processes if it exceeds reasonable limits or text is short
        if num_processes > text_length: # Avoid excessive processes for short text
             num_processes = max(1, text_length // 100) # Heuristic: 1 process per 100 chars? Adjust.
             logging.info(f"Reduced num_processes to {num_processes} due to short text length.")
        if num_processes <= 0:
             num_processes = 1
             logging.info("Set num_processes to 1.")


        # Calculate samples per process and text chunk length for distribution
        # Use ceiling division to ensure all samples are targeted
        samples_per_process = (total_samples + num_processes - 1) // num_processes
        # Distribute text roughly evenly - ensure all text is covered
        chunk_indices = [ (i * text_length) // num_processes for i in range(num_processes + 1)]
        text_chunks = [ all_text[chunk_indices[i]:chunk_indices[i+1]] for i in range(num_processes) ]

        logging.info(f"Distributing work to {num_processes} processes. Target samples/process: ~{samples_per_process}")

        # Prepare arguments for starmap
        starmap_args = [
            (
                text_chunks[i],
                samples_per_process,
                max_tokens_per_sample,
                internal_processing_chunk_size,
                tokenizer,
            )
            for i in range(num_processes)
        ]

        results = []
        # Use try-finally to ensure pool closure
        pool = multiprocessing.Pool(processes=num_processes)
        try:
            # Using starmap requires a single function that takes tuple arguments
            # results = pool.starmap(process_chunk_wrapper, starmap_args) # Use starmap if wrapper takes tuple
            # Or use map with a wrapper that takes the tuple
            results = pool.map(process_chunk_wrapper, starmap_args)
            logging.info("Finished processing sample generation chunks in parallel.")
        except Exception as e:
            logging.error(f"Error during multiprocessing pool execution for sample generation: {e}")
            return [] # Return empty list on pool error
        finally:
            pool.close()
            pool.join()

        # Concatenate results from all processes
        all_samples = [sample for result_list in results for sample in result_list]
        logging.info(f"Collected results. Total samples generated: {len(all_samples)}")

        # Optional: Truncate if slightly more than total_samples were generated due to rounding
        if len(all_samples) > total_samples:
            logging.info(f"Truncating sample list from {len(all_samples)} to target {total_samples}.")
            all_samples = all_samples[:total_samples]

    except Exception as e:
        logging.error(f"An unexpected error occurred in distribute_processing: {e}")
        return [] # Return empty list on unexpected errors

    return all_samples
